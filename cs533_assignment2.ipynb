{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS533 Assignment 2\n",
        "---\n",
        "### Course No: CS533\n",
        "### Assingment No: 2\n",
        "\n",
        "<br>\n",
        "\n",
        "### Student Name: Hasan Alp Caferoğlu\n",
        "### Student ID: 22203991\n",
        "### Email Address:  alp.caferoglu@bilkent.edu.tr\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-1l8Lqf0drj4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1xBTQGv5nuq",
        "outputId": "97a6469a-8a81-462b-d944-1a434e17449b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank-bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank-bm25) (1.23.5)\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2.2\n",
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.41)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.10)\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.2\n",
            "time: 344 µs (started: 2023-11-26 17:00:32 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install rank-bm25\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_Mf-OvI7lH1",
        "outputId": "37229aa4-fb63-4380-f590-af6150509021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "time: 19.8 s (started: 2023-11-26 17:00:32 +00:00)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "CISI_documents_path = '/content/drive/MyDrive/1.MS/CS533_IRS/assignments/assignment2/implementation/CISI/documents.csv'\n",
        "CISI_queries_path = '/content/drive/MyDrive/1.MS/CS533_IRS/assignments/assignment2/implementation/CISI/queries.csv'\n",
        "CISI_ground_truth_path = '/content/drive/MyDrive/1.MS/CS533_IRS/assignments/assignment2/implementation/CISI/ground_truth.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIuSGpy44XiY",
        "outputId": "877e022c-b87c-4481-cd08-26c3ba3e4832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.91 s (started: 2023-11-26 17:00:52 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Downloading NLTK resources\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywvomlVT5cxZ",
        "outputId": "cf2c8b6f-705d-4cff-c0f8-f32866e541b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.51 s (started: 2023-11-26 17:00:56 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Reading CSV files into Pandas DataFrames\n",
        "documents_df = pd.read_csv(CISI_documents_path)\n",
        "queries_df = pd.read_csv(CISI_queries_path)\n",
        "ground_truth_df = pd.read_csv(CISI_ground_truth_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_gTdtSixOqY",
        "outputId": "d65f4f0a-de9e-44b7-b7bd-7d5f07cd80af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 678 µs (started: 2023-11-26 17:00:58 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Tokenization function\n",
        "def tokenizerFunction(textData):\n",
        "  \"\"\"\n",
        "    Splitting of a sentence into its words and returns list of words\n",
        "\n",
        "    Parameters:\n",
        "    - textData: list, sentence that will be tokenized\n",
        "\n",
        "    Returns:\n",
        "    - tokens, list of tokens\n",
        "  \"\"\"\n",
        "  tokens = textData.split()\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NGPSqBn-c6v",
        "outputId": "c34780b8-226d-4056-fb81-db8583a68f2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.63 ms (started: 2023-11-26 17:00:58 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing function\n",
        "def preprocessFunction(textData):\n",
        "  \"\"\"\n",
        "    Applying preprocessing to the given sentence.\n",
        "    Preprocessing involves removing stop words, removing punctuation marks\n",
        "    and lowercasing the text.\n",
        "\n",
        "    Parameters:\n",
        "    - textData: list, sentence that will be preprocessed\n",
        "\n",
        "    Returns:\n",
        "    - processedTextData as string\n",
        "  \"\"\"\n",
        "  # Converting text to lowercase\n",
        "  if isinstance(textData, str):\n",
        "    textData = textData.lower()\n",
        "\n",
        "    # Removing punctuations\n",
        "    # textData = textData.translate(str.maketrans('', '', string.punctuation)) # converts \"don't\" to \"dont\"\n",
        "    textData = textData.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))) # converts \"don't\" to \"don t\"\n",
        "\n",
        "    # Removing stopwords\n",
        "    stopWordsSet = set(stopwords.words('english'))\n",
        "    tokens = tokenizerFunction(textData)\n",
        "    tokens = [token for token in tokens if token not in stopWordsSet]\n",
        "\n",
        "    processedTextData = ' '.join(tokens)\n",
        "    return processedTextData\n",
        "  else:\n",
        "    # if the text data is NaN return empty string\n",
        "    return ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eulR48NSk3tt",
        "outputId": "bed09ea1-5fc5-4b2e-82ab-4b28e09d8f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.27 s (started: 2023-11-26 17:00:58 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing 'text' column in each DataFrame\n",
        "documents_df['processedText'] = documents_df['text'].apply(preprocessFunction)\n",
        "documents_df['processedTitle'] = documents_df['title'].apply(preprocessFunction)\n",
        "queries_df['processedText'] = queries_df['text'].apply(preprocessFunction)\n",
        "\n",
        "# Storing tokens of processed text\n",
        "documents_df['processedTextTokens'] = documents_df['processedText'].apply(tokenizerFunction)\n",
        "documents_df['processedTitleTokens'] = documents_df['processedTitle'].apply(tokenizerFunction)\n",
        "queries_df['processedTextTokens'] = queries_df['processedText'].apply(tokenizerFunction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC-k5iAE7_5H",
        "outputId": "904e5a1d-6ab4-47be-e271-6c7c8ea6ad59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.17 ms (started: 2023-11-26 17:00:59 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Combining processed document titles, document texts and query texts for train set\n",
        "trainDataSetTokenized =  documents_df['processedTitleTokens'].tolist() + documents_df['processedTextTokens'].tolist() + queries_df['processedTextTokens'].tolist()\n",
        "trainDataSet = documents_df['processedTitle'].tolist() + documents_df['processedText'].tolist() + queries_df['processedText'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o99ucd2O6JLz",
        "outputId": "bc494c38-defc-49dd-b21d-26fd1e4652a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5058147, 5495550)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 46 s (started: 2023-11-26 17:00:59 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Initializing Gensim Word2Vec model\n",
        "model = Word2Vec(\n",
        "    window=10, # 10 words before the target word and 10 words after it\n",
        "    min_count=2, # at least 2 words must be present in the sentence\n",
        "    workers=2, # number of used cpu threads for training,\n",
        "    vector_size=300, # vector size is set to 300 to equalize with the google model\n",
        "    epochs=50\n",
        ")\n",
        "\n",
        "# Building vocabulariy (building list of unique words)\n",
        "model.build_vocab(trainDataSetTokenized)\n",
        "model.train(trainDataSetTokenized,\n",
        "            total_examples=model.corpus_count,\n",
        "            epochs=model.epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DvYKY1yQQFma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb35d6be-7f38-4cf2-a275-31a4778b6f72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "time: 8min 59s (started: 2023-11-26 17:01:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Downlading pre-trained model \"word2vec-google-news-300\"\n",
        "preTrainedGoogleNewsModel = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZGQA6nK8pta",
        "outputId": "a404ec5b-1951-4fb1-bd6e-c063b99da12d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.09 ms (started: 2023-11-26 17:10:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def embeddingComputationFunction(nlTokens, wvModel):\n",
        "  \"\"\"\n",
        "    Deriving the embedding for an input (a query or a document) by averaging\n",
        "    the embeddings of its constituent words.\n",
        "\n",
        "    Parameters:\n",
        "    - nlTokens: list, the tokens of input query or document for which to compute the embedding.\n",
        "    - wvModel: KeyedVectors, model that is used for computing embeddings\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray, embedding vector for a given input.\n",
        "  \"\"\"\n",
        "\n",
        "  # Filtering words that are not present in the model\n",
        "  nlTokens = [token for token in nlTokens if token in wvModel]\n",
        "\n",
        "  # Returning zero vector if there is no words that present in the model\n",
        "  if not nlTokens:\n",
        "    return np.zeros(wvModel.vector_size)\n",
        "\n",
        "  tokensEmbeddingList = [wvModel[token] for token in nlTokens]\n",
        "  # Note that average embedding is found by averaging along the column\n",
        "  avgEmbedding = np.mean(tokensEmbeddingList, axis=0)\n",
        "\n",
        "  return avgEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tbosPzyEFuas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad92a574-94d8-4ad2-cd74-b3436b28a500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 672 ms (started: 2023-11-26 17:10:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# instantiate the TF-IDF vectorizer object\n",
        "tfidfVectorizer = TfidfVectorizer()\n",
        "# Learning vocabulary and idf\n",
        "tfidfVectorizer.fit(trainDataSet)\n",
        "tfidfTrainSetVector = tfidfVectorizer.transform(trainDataSet) # tfidfVector is scipy sparse matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-3qqRDlYmk5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4110412-39b6-4a06-d023-1a3a9c8b6760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 119 ms (started: 2023-11-26 17:10:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# BM25 Initializing\n",
        "bm25 = BM25Okapi(documents_df['processedTextTokens'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "V74IoYwBua_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da8a23f4-81c7-44b7-e65e-2bf6cae1009b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15873646896054894"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 21.4 ms (started: 2023-11-26 17:10:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Calculating TF-IDF score for query-document pair\n",
        "def tfidfScoreQueryDocPair(query, doc):\n",
        "  \"\"\"\n",
        "    Obtaining tfidf score for a document query pair\n",
        "\n",
        "    Parameters:\n",
        "    - query: string, considered query\n",
        "    - docId: string, considered doc\n",
        "\n",
        "    Returns:\n",
        "    - float, tfidf score for query-document pair\n",
        "  \"\"\"\n",
        "  tfidfScore = np.dot(tfidfVectorizer.transform([doc]).toarray(), tfidfVectorizer.transform([query]).toarray().T).sum()\n",
        "  return tfidfScore\n",
        "\n",
        "# # Example for TF-IDF score\n",
        "# tfidfScoreQueryDocPair(queries_df['processedText'][0], documents_df['processedText'][758])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6GSMHHvEH5ET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71e3ed61-970f-4e42-8d24-b619c9339eac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8012794563366724\n",
            "time: 42.3 ms (started: 2023-11-26 17:10:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def bm25ScoreQueryDocPair(query, docId):\n",
        "  \"\"\"\n",
        "    Obtaining BM25 score for a document query pair\n",
        "\n",
        "    Parameters:\n",
        "    - query: string, the query which related documents wil be retrieved\n",
        "    - docId: int, document ID that is compared with given query\n",
        "\n",
        "    Returns:\n",
        "    - float, BM25 score for query-document pair\n",
        "  \"\"\"\n",
        "  scoreList = bm25.get_scores(query.split())\n",
        "  # normalization of score is done by dividing obtained score with max score\n",
        "  bm25Score = scoreList[docId-1] / max(scoreList)\n",
        "  return bm25Score\n",
        "\n",
        "# # Example for BM25 Score\n",
        "# bm25score = bm25ScoreQueryDocPair(queries_df['processedText'][0], 759)\n",
        "# print(bm25score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "a5uroV-Xny2j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0650013f-3315-4c99-ac49-01985ea2efbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6386269\n",
            "0.68639153\n",
            "time: 14.6 ms (started: 2023-11-26 17:10:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def embeddingScoreQueryDocPair(query, doc, model):\n",
        "  \"\"\"\n",
        "    Obtaining embedding score for a document query pair\n",
        "\n",
        "    Parameters:\n",
        "    - query: string, considered query\n",
        "    - doc: string, considered doc\n",
        "    - model: word2vec model that is wanted to be used\n",
        "\n",
        "    Returns:\n",
        "    - float, embedding score for query-document pair\n",
        "  \"\"\"\n",
        "  queryEmbeddings = embeddingComputationFunction(query.split(), model)\n",
        "  docEmbeddings = embeddingComputationFunction(doc.split(), model)\n",
        "  # applying cosine similarity\n",
        "  if (np.linalg.norm(queryEmbeddings) * np.linalg.norm(docEmbeddings)) != 0:\n",
        "    embeddingScore = np.dot(queryEmbeddings, docEmbeddings) / (np.linalg.norm(queryEmbeddings) * np.linalg.norm(docEmbeddings))\n",
        "    return embeddingScore\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "# # Example for embedding score\n",
        "# embeddingScore = embeddingScoreQueryDocPair(queries_df['processedText'][0], documents_df['processedText'][758], model.wv)\n",
        "# print(embeddingScore)\n",
        "# embeddingScore = embeddingScoreQueryDocPair(queries_df['processedText'][0], documents_df['processedText'][758], preTrainedGoogleNewsModel)\n",
        "# print(embeddingScore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3XA23FjTmvGr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acf9bd11-4f3b-4670-f9d3-b15113f16ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5275521236008373\n",
            "0.5433144610470501\n",
            "time: 124 ms (started: 2023-11-26 17:10:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def combinedScoreQueryDocPair(doc, query, docId, embeddingModel, tfidfWeight, bm25Weight, embeddingWeight):\n",
        "  \"\"\"\n",
        "    Obtaining total combined score for a document query pair\n",
        "\n",
        "    Parameters:\n",
        "    - doc: string, context of the doc\n",
        "    - query: string, query text\n",
        "    - docId: int, ID of considered doc\n",
        "    - embeddingModel: word2vec model that is wanted to be used\n",
        "    - tfidfWeight: float, weight for tfidf score\n",
        "    - bm25Weight: float, weight for bm25 score\n",
        "    - embeddingWeight: float, weight for embedding score\n",
        "\n",
        "    Returns:\n",
        "    - float, combined score for query-document pair\n",
        "  \"\"\"\n",
        "\n",
        "  tfidfScore = tfidfScoreQueryDocPair(query, doc)\n",
        "  bm25Score = bm25ScoreQueryDocPair(query, docId)\n",
        "  embeddingScore = embeddingScoreQueryDocPair(query, doc, embeddingModel)\n",
        "  combinedScore = tfidfWeight * tfidfScore + bm25Weight * bm25Score + embeddingWeight * embeddingScore\n",
        "  return combinedScore\n",
        "\n",
        "# # Example for combined score\n",
        "# docId = 759\n",
        "# queryId = 1\n",
        "# doc = documents_df['processedText'][docId-1]\n",
        "# query = queries_df['processedText'][queryId-1]\n",
        "# combinedScore = combinedScoreQueryDocPair(doc, query, docId, model.wv, 0.33, 0.33, 0.33)\n",
        "# print(combinedScore)\n",
        "# combinedScore = combinedScoreQueryDocPair(doc, query, docId, preTrainedGoogleNewsModel, 0.33, 0.33, 0.33)\n",
        "# print(combinedScore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "maBtRdeowVZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b622c4de-aee4-4da5-ec12-8b94f84deb72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.23 ms (started: 2023-11-26 17:10:46 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def retrievalSystem(docDataFrame, queryDataFrame, queryId, embeddingModel, tfidfWeight, bm25Weight, embeddingWeight):\n",
        "  docScores = []\n",
        "  query = queryDataFrame['processedText'][queryId-1]\n",
        "  for docIndex in range(len(docDataFrame)):\n",
        "    docId = docIndex+1\n",
        "    doc = docDataFrame['processedText'][docId-1]\n",
        "    combinedScore = combinedScoreQueryDocPair(doc, query, docId, embeddingModel, tfidfWeight, bm25Weight, embeddingWeight)\n",
        "    docScores.append((docId, combinedScore))\n",
        "\n",
        "  sortedDocScores = sorted(docScores, key=lambda x: x[1], reverse=True)\n",
        "  top10 = [item[0] for item in sortedDocScores[:10]]\n",
        "  return top10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "418nLkpERQv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dee0f70-04e4-4c30-c351-7fe8f6af4960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.95 ms (started: 2023-11-26 21:25:41 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def calculateMAPForSystem(groundTruthDataFrame, docDataFrame, queryDataFrame, embeddingModel, tfidfWeight, bm25Weight, embeddingWeight):\n",
        "\n",
        "  \"\"\"\n",
        "    Calculating MAP for a specific system\n",
        "\n",
        "    Parameters:\n",
        "    - groundTruthDataFrame: DataFrame, DataFrame constructed from ground truth\n",
        "    - docDataFrame: DataFrame, DataFrame constructed from documents\n",
        "    - queryDataFrame: DataFrame, DataFrame constructed from queries\n",
        "    - embeddingModel: word2vec model that is wanted to be used\n",
        "    - tfidfWeight: float, weight for tfidf score\n",
        "    - bm25Weight: float, weight for bm25 score\n",
        "    - embeddingWeight: float, weight for embedding score\n",
        "\n",
        "    Returns:\n",
        "    - float, MAP score for a system\n",
        "  \"\"\"\n",
        "  # applying MAP at 10\n",
        "  avgPrecisionForQueries = []\n",
        "  # Obtaining queries and count of relevant documents\n",
        "  queryAndRelDocNum =  groundTruthDataFrame['query_id'].value_counts().reset_index().rename(columns={'query_id': 'rel_doc_count', 'index': 'query_id'})\n",
        "  queryAndRelDocNum=queryAndRelDocNum.sort_values(by='query_id').reset_index(drop=True)\n",
        "  for qIndex in range(len(queryAndRelDocNum)):\n",
        "    queryId = queryAndRelDocNum.loc[qIndex, 'query_id']\n",
        "    relevantDocIds = (groundTruthDataFrame[groundTruthDataFrame['query_id'] == queryId])['doc_id'].tolist()\n",
        "    top10RetrievedDocIds = retrievalSystem(docDataFrame, queryDataFrame, queryId, embeddingModel, tfidfWeight, bm25Weight, embeddingWeight)\n",
        "    # print('top 10 retrieved docs for query id ', queryId, \": \")\n",
        "    # print(top10RetrievedDocIds)\n",
        "\n",
        "    relevanceCount = 0\n",
        "    precisionSum = 0\n",
        "    for i in range(len(top10RetrievedDocIds)):\n",
        "      retrievedDocId = top10RetrievedDocIds[i]\n",
        "      if retrievedDocId in relevantDocIds:\n",
        "        relevanceCount+=1\n",
        "        precisionSum += relevanceCount / (i+1)\n",
        "\n",
        "    avgPrecision = precisionSum / len(relevantDocIds)\n",
        "    print('Average Precision for query id ', queryId, \": \", avgPrecision)\n",
        "    avgPrecisionForQueries.append(avgPrecision)\n",
        "    print(\"-\"*30)\n",
        "\n",
        "  print(\" Sum of average precisions: \", sum(avgPrecisionForQueries))\n",
        "  print(\"Number of queries considered: \", len(avgPrecisionForQueries))\n",
        "  MAP = sum(avgPrecisionForQueries) / len(avgPrecisionForQueries)\n",
        "  print('MAP for the system: ', MAP)\n",
        "  print(\"-\"*30)\n",
        "  return MAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CYugSvh_S1-C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "95be3025-92e9-436f-90ae-8d1df843f816"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-24f57525a3a1>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mtfidfWeight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbm25Weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddingWeight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0membeddingModelNo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embeddingModel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mMAP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculateMAPForSystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddingModels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membeddingModelNo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidfWeight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbm25Weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddingWeight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m   \u001b[0msystem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MAP'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'System {systemId}: \\n MAP: {MAP} Weights: {weights} Embedding Model: {embeddingModels[embeddingModelNo]} \\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-cba929541580>\u001b[0m in \u001b[0;36mcalculateMAPForSystem\u001b[0;34m(groundTruthDataFrame, docDataFrame, queryDataFrame, embeddingModel, tfidfWeight, bm25Weight, embeddingWeight)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mqueryId\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueryAndRelDocNum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'query_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrelevantDocIds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgroundTruthDataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroundTruthDataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mqueryId\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtop10RetrievedDocIds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrievalSystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueryDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueryId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddingModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidfWeight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbm25Weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddingWeight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# print('top 10 retrieved docs for query id ', queryId, \": \")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# print(top10RetrievedDocIds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-5ad06b9ba06b>\u001b[0m in \u001b[0;36mretrievalSystem\u001b[0;34m(docDataFrame, queryDataFrame, queryId, embeddingModel, tfidfWeight, bm25Weight, embeddingWeight)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdocId\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocIndex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocDataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processedText'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocId\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcombinedScore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombinedScoreQueryDocPair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddingModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidfWeight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbm25Weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddingWeight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdocScores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombinedScore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-570ee9edb62b>\u001b[0m in \u001b[0;36mcombinedScoreQueryDocPair\u001b[0;34m(doc, query, docId, embeddingModel, tfidfWeight, bm25Weight, embeddingWeight)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mtfidfScore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidfScoreQueryDocPair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mbm25Score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm25ScoreQueryDocPair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0membeddingScore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddingScoreQueryDocPair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddingModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mcombinedScore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidfWeight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtfidfScore\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbm25Weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbm25Score\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membeddingWeight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0membeddingScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-2818fee33c7a>\u001b[0m in \u001b[0;36mbm25ScoreQueryDocPair\u001b[0;34m(query, docId)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m-\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBM25\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdocument\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \"\"\"\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mscoreList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm25\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0;31m# normalization of score is done by dividing obtained score with max score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mbm25Score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscoreList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocId\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoreList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/rank_bm25.py\u001b[0m in \u001b[0;36mget_scores\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mdoc_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mq_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_freqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n\u001b[1;32m    120\u001b[0m                                                (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.23 s (started: 2023-11-26 21:25:42 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Setting path for printed result.txt file\n",
        "systemPerformanceFilePath = '/content/drive/MyDrive/1.MS/CS533_IRS/assignments/assignment2/implementation/resutls.txt'\n",
        "# Setting weights to create different configurations\n",
        "# Note that system will automatically measure MAP for a configuration both using pre-trained word2vec model and model trained on CISI\n",
        "systemsWeightList = [(0.4, 0.25, 0.35), (0.4, 0.25, 0.35)] # List of set of weights\n",
        "\n",
        "\n",
        "# Constant embedding model list\n",
        "embeddingModels = [model.wv, preTrainedGoogleNewsModel]\n",
        "\n",
        "systemsDict = {}\n",
        "sysCount = 1\n",
        "for index in range(len(systemsWeightList)):\n",
        "  system = {'embeddingModel': 0, 'weights': systemsWeightList[index]}\n",
        "  systemsDict[f'{sysCount}'] = system\n",
        "  sysCount+=1\n",
        "\n",
        "  # if embeddingWeight is larger than 0, there must 2 systems with both our trained model and pretrained model\n",
        "  if systemsWeightList[index][2] > 0:\n",
        "    systemWithAnotherModel = {'embeddingModel': 1, 'weights': systemsWeightList[index]}\n",
        "    systemsDict[f'{sysCount}'] = systemWithAnotherModel\n",
        "    sysCount+=1\n",
        "\n",
        "\n",
        "# print(systemsDict)\n",
        "# print(len(systemsDict))\n",
        "for systemId in range(1, len(systemsDict)+1):\n",
        "  system = systemsDict[f'{systemId}']\n",
        "  weights = system['weights']\n",
        "  tfidfWeight, bm25Weight, embeddingWeight = system['weights']\n",
        "  embeddingModelNo = system['embeddingModel']\n",
        "  MAP = calculateMAPForSystem(ground_truth_df, documents_df, queries_df, embeddingModels[embeddingModelNo], tfidfWeight, bm25Weight, embeddingWeight)\n",
        "  system['MAP'] = MAP\n",
        "  print(f'System {systemId}: \\n MAP: {MAP} Weights: {weights} Embedding Model: {embeddingModels[embeddingModelNo]} \\n')\n",
        "\n",
        "\n",
        "# Printing System Performances into txt file\n",
        "with open(systemPerformanceFilePath, 'w') as fileHandle:\n",
        "  for systemId, system in systemsDict.items():\n",
        "    weights = system['weights']\n",
        "    MAP = system['MAP']\n",
        "    embeddingModelId = system['embeddingModel']\n",
        "    if embeddingModelId == 1:\n",
        "      embeddingModel = 'Trained Model with CISI Dataset'\n",
        "    else:\n",
        "      embeddingModel = 'word2vec-google-news-300'\n",
        "    line = f'System {systemId}: \\n MAP: {MAP} Weights: {weights} Embedding Model: {embeddingModel} \\n'\n",
        "    fileHandle.write(line)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NH317jfvYypX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}